{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRkmRNdUaq-w"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi5HddpkZIMN"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax\n",
        "import optax\n",
        "import random\n",
        "import math\n",
        "\n",
        "import jax.random as jrandom\n",
        "import jax.lax as lax\n",
        "import flax.linen as nn\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from flax.training import train_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q49q0LUa-Ft"
      },
      "source": [
        "## Stochastic MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN_0eAe-a_pR"
      },
      "outputs": [],
      "source": [
        "class StochasticMDP:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.end = False\n",
        "    self.current_state = 2\n",
        "    self.num_actions = 2\n",
        "    self.num_states = 6\n",
        "    self.p_right = 0.5\n",
        "\n",
        "  def reset(self):\n",
        "    self.end = False\n",
        "    self.current_state = 2\n",
        "    state = jnp.zeros(self.num_states)\n",
        "    state.at[self.current_state - 1].set(1)\n",
        "    return state\n",
        "\n",
        "  def step(self, action):\n",
        "    if self.current_state != 1:\n",
        "      if action == 1:\n",
        "        if random.random() < self.p_right and self.current_state < self.num_states:\n",
        "          self.current_state += 1\n",
        "        else:\n",
        "          self.current_state -= 1\n",
        "      if action == 0:\n",
        "        self.current_state -= 1\n",
        "      if self.current_state == self.num_states:\n",
        "        self.end = True\n",
        "\n",
        "    state = jnp.zeros(self.num_states)\n",
        "    state.at[self.current_state - 1].set(1)\n",
        "    if self.current_state == 1:\n",
        "      if self.end:\n",
        "        return lax.stop_gradient(state), 1.00, True, {}\n",
        "      else:\n",
        "        return lax.stop_gradient(state), 1/100, True, {}\n",
        "    else:\n",
        "      return lax.stop_gradient(state), 0, False, {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssp_QTUCeXL2"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwsroO6NegEn"
      },
      "outputs": [],
      "source": [
        "Batch = namedtuple(\n",
        "    \"Batch\",\n",
        "    [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
        ")\n",
        "\n",
        "def to_onehot(x):\n",
        "  oh = jnp.zeros(6)\n",
        "  oh.at[x - 1].set(1)\n",
        "  return oh\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.capacity = capacity\n",
        "    self.buffer = deque(maxlen = capacity)\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "    state = jnp.expand_dims(state, 0)\n",
        "    next_state = jnp.expand_dims(next_state, 0)\n",
        "    self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def expand(self, x):\n",
        "    return [i[jnp.newaxis] for i in x]\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "    action = self.expand(action)\n",
        "    reward = self.expand(reward)\n",
        "    done = self.expand(done)\n",
        "    return Batch(\n",
        "        state = jnp.concatenate(state),\n",
        "        action = jnp.concatenate(action),\n",
        "        reward = jnp.concatenate(reward),\n",
        "        next_state = jnp.concatenate(next_state),\n",
        "        done = jnp.concatenate(done)\n",
        "    )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNL6L3evilpp"
      },
      "source": [
        "## High and Low Level Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9ipE-g-iowk"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "  hidden_dim: int\n",
        "  out_dim: int\n",
        "  activation: any = nn.relu\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train = True):\n",
        "    x = nn.Dense(self.hidden_dim)(x)\n",
        "    x = self.activation(x)\n",
        "    x = nn.Dense(self.out_dim)(x)\n",
        "    if not train:\n",
        "      x = jnp.argmax(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvosgYVwka1p"
      },
      "source": [
        "## Create Train State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qSadmVXkcRn"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, model, input_dim, learning_rate):\n",
        "  dummy_input = jnp.ones((1, input_dim))\n",
        "  params = model.init(rng, dummy_input)[\"params\"]\n",
        "  tx = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn = model.apply,\n",
        "      params = params,\n",
        "      tx = tx\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvZkJHyFlBY2"
      },
      "source": [
        "## hDQN Update Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay4bSm_XqztP"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def get_action(train_state, state):\n",
        "  action = train_state.apply_fn({\"params\": train_state.params}, state, train = False)\n",
        "  return lax.stop_gradient(action)\n",
        "\n",
        "@jax.jit\n",
        "def get_goal(train_state, state):\n",
        "  action = train_state.apply_fn({\"params\": train_state.params}, state, train = False)\n",
        "  return lax.stop_gradient(action)\n",
        "\n",
        "@jax.jit\n",
        "def update_train_state(train_state, batch):\n",
        "  state, action, reward, next_state, done = batch.state, batch.action, batch.reward, batch.next_state, batch.done\n",
        "\n",
        "  def loss_fn(params):\n",
        "    outs = train_state.apply_fn({\"params\": train_state.params}, state)\n",
        "    q_vals = outs[jnp.arange(action.shape[0]), action]\n",
        "    next_outs = train_state.apply_fn({\"params\": train_state.params}, next_state)\n",
        "    next_q_vals = jnp.max(next_outs, axis = 1)\n",
        "    exp_q_vals = lax.stop_gradient(reward + 0.99 * next_q_vals * (1 - done))\n",
        "    loss = jnp.mean(jnp.square(q_vals - exp_q_vals))\n",
        "    return loss\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn)\n",
        "  _, grads = grad_fn(train_state.params)\n",
        "  new_train_state = train_state.apply_gradients(grads = grads)\n",
        "  return new_train_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyyuX14Uq34I"
      },
      "source": [
        "## hDQN Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGDVObr4lqoH"
      },
      "outputs": [],
      "source": [
        "class hDQN:\n",
        "  def __init__(self, num_goals, num_actions, lr, hidden_dim, batch_size):\n",
        "    self.num_goals = num_goals\n",
        "    self.num_actions = num_actions\n",
        "    self.lr = lr\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    rng = jrandom.PRNGKey(42)\n",
        "    self.rng, model_key, meta_key = jrandom.split(rng, 3)\n",
        "    self.model = Actor(hidden_dim = self.hidden_dim, out_dim = self.num_actions)\n",
        "    self.model_state = create_train_state(model_key, self.model, 2 * self.num_goals, self.lr)\n",
        "    self.meta_model = Actor(hidden_dim = self.hidden_dim, out_dim = self.num_goals)\n",
        "    self.meta_model_state = create_train_state(meta_key, self.meta_model, self.num_goals, self.lr)\n",
        "\n",
        "  def update_learner(self, buffer):\n",
        "    batch = buffer.sample(self.batch_size)\n",
        "    self.model_state = update_train_state(self.model_state, batch)\n",
        "\n",
        "  def update_meta_learner(self, meta_buffer):\n",
        "    batch = meta_buffer.sample(self.batch_size)\n",
        "    self.meta_model_state = update_train_state(self.meta_model_state, batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGICH09nsSDX"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFJ1y7PusUAC"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  hidden_dim = 256\n",
        "  batch_size = 32\n",
        "  learning_rate = 1e-3\n",
        "  eval_interval = 500\n",
        "  frame_start = 1.0\n",
        "  frame_end = 0.01\n",
        "  frame_decay = 500\n",
        "  steps = 20000\n",
        "\n",
        "  env = StochasticMDP()\n",
        "  num_goals = env.num_states\n",
        "  num_actions = env.num_actions\n",
        "  replay_buffer = ReplayBuffer(10000)\n",
        "  meta_replay_buffer = ReplayBuffer(10000)\n",
        "\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  all_rewards = []\n",
        "  episode_reward = 0\n",
        "  frame_idx = 1\n",
        "  epsilon_by_frame = lambda frame_idx: frame_end + (frame_start - frame_end) * math.exp(-1 * frame_idx / frame_decay)\n",
        "  hdqn = hDQN(num_goals, num_actions, learning_rate, hidden_dim, batch_size)\n",
        "\n",
        "  while frame_idx < steps:\n",
        "    eps = epsilon_by_frame(frame_idx)\n",
        "    goal = get_goal(hdqn.meta_model_state, state)\n",
        "    if random.random() <= eps:\n",
        "      goal = jnp.array(random.randrange(num_goals))\n",
        "    onehot_goal = to_onehot(goal)\n",
        "    meta_state = state\n",
        "    extrinsic_reward = 0\n",
        "    while not done and goal != np.argmax(state):\n",
        "      goal_state = np.concatenate([state, onehot_goal])\n",
        "      action = get_action(hdqn.model_state, goal_state)\n",
        "      if random.random() < eps:\n",
        "        action = jnp.array(random.randrange(num_actions))\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      episode_reward += reward\n",
        "      extrinsic_reward += reward\n",
        "      intrinsic_reward = jnp.array(1.0 if goal == np.argmax(next_state) else 0)\n",
        "\n",
        "      replay_buffer.push(\n",
        "          goal_state, action, intrinsic_reward, np.concatenate([next_state, onehot_goal]), jnp.array(done)\n",
        "      )\n",
        "      state = next_state\n",
        "      if len(replay_buffer) >= batch_size:\n",
        "        hdqn.update_learner(replay_buffer)\n",
        "      if len(meta_replay_buffer) >= batch_size:\n",
        "        hdqn.update_meta_learner(meta_replay_buffer)\n",
        "      frame_idx += 1\n",
        "      if frame_idx % eval_interval == 0:\n",
        "        print(f\"Step {frame_idx} | Return {all_rewards[-1]}\")\n",
        "    meta_replay_buffer.push(meta_state, goal, jnp.array(extrinsic_reward), state, jnp.array(done))\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "      all_rewards.append(episode_reward)\n",
        "      episode_reward = 0\n",
        "\n",
        "  n = 100\n",
        "  ret_mean = [np.mean(all_rewards[i:i + n]) for i in range(0, len(all_rewards), n)]\n",
        "  plt.figure(figsize = (10, 5))\n",
        "  plt.title(\"StochasticMDP\")\n",
        "  plt.plot(ret_mean)\n",
        "  plt.ylabel(\"Average Return\")\n",
        "  plt.xlabel(\"Episodes (x 1000)\")\n",
        "  plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2VzapPTzKoW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}