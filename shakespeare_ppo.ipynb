{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    action_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, state):\n",
    "    x = nn.Dense(256)(state)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(256)(x)\n",
    "    x = nn.relu(x)\n",
    "\n",
    "    logits = nn.Dense(self.action_dim)(x)\n",
    "    value = nn.Dense(1)(x)\n",
    "\n",
    "    return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def sample_actions(rng, logits):\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return jax.random.categorical(rng, logits), log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Advantages and Reward-To-Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, values, dones, last_value, gamma):\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for i in reversed(len(rewards)):\n",
    "        discounted_reward = rewards[i] + gamma * discounted_reward * dones[i]\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = np.array(returns)\n",
    "    advantages = returns - values\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng, model, input_dim, action_dim, learning_rate):\n",
    "    dummy_input = jnp.ones((1, input_dim))\n",
    "    params = model.init(rng, dummy_input)\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState(\n",
    "        apply_fn = model.apply,\n",
    "        params = params,\n",
    "        tx = tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnums = (5, 6, 7))\n",
    "def train_step(\n",
    "    state,\n",
    "    states,\n",
    "    actions,\n",
    "    old_log_probs,\n",
    "    advantages,\n",
    "    returns,\n",
    "    clip_eps,\n",
    "    max_grad_norm\n",
    "):\n",
    "    def loss_fn(params):\n",
    "        mean, values = state.apply_fn(params, states)\n",
    "        log_probs = jax.nn.log_softmax(mean)\n",
    "        \n",
    "        ratio = jnp.exp(log_probs - old_log_probs)\n",
    "        clipped_ratio = jnp.clip(ratio, 1 - clip_eps, 1 + clip + eps)\n",
    "        loss1 = ratio * advantages\n",
    "        loss2 = clipped_ratio * advantages\n",
    "        policy_loss = -jnp.mean(jnp.minimum(loss1, loss2))\n",
    "\n",
    "        value_loss = jnp.mean((values.squeeze() - returns) ** 2)\n",
    "\n",
    "        return policy_loss + value_loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    _, grads = grad_fn(state.params)\n",
    "    grads, _ = optax.clip_by_global_norm(grads, max_grad_norm)\n",
    "    new_state = state.apply_gradients(grads)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def update_ppo(state, obs, batch_size, num_minibatches, clip_eps, max_grad_norm):\n",
    "    indices = jnp.arange(len(obs[\"states\"]))\n",
    "    indices = jax.random.permutation(jax.random.PRNGKey(42), indices)\n",
    "\n",
    "    for _ in range(num_minibatches):\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            mb_indices = indices[i: i + batch_size]\n",
    "            mb_states = obs[\"states\"][mb_indices]\n",
    "            mb_actions = obs[\"actions\"][mb_indices]\n",
    "            mb_old_logprobs = obs[\"log_probs\"][mb_indices]\n",
    "            mb_advantages = obs[\"advantages\"][mb_indices]\n",
    "            mb_returns = obs[\"returns\"][mb_indices]\n",
    "\n",
    "            mb_advantages = (mb_advantages - jnp.mean(advantages)) / jnp.std(advantages) + 1e-8\n",
    "            state = train_step(state, mb_states, mb_actions, mb_old_logprobs, mb_advantages, mb_returns, clip_eps, max_grad_norm)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def collect_trajectories(env, state, rng, steps_per_epoch, gamma):\n",
    "    buffer = {\n",
    "        \"states\": [],\n",
    "        \"actions\": [],\n",
    "        \"rewards\": [],\n",
    "        \"dones\": [],\n",
    "        \"values\": [],\n",
    "        \"log_probs\": []\n",
    "    }\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    epsiode_return = 0\n",
    "    episode_length = 0\n",
    "    for _ in range(steps_per_epoch):\n",
    "        rng, actions_rng = jax.random.split(rng)\n",
    "        mean, value = state.apply_fn(state.params, jnp.array([obs]))\n",
    "        action, log_prob = sample_actions(action_rng, mean)\n",
    "\n",
    "        next_obs, reward, done, _ = env.step(np.array(action))\n",
    "\n",
    "        buffer[\"states\"].append(obs)\n",
    "        buffer[\"actions\"].append(action)\n",
    "        buffer[\"rewards\"].append(reward)\n",
    "        buffer[\"dones\"].append(done)\n",
    "        buffer[\"values\"].append(value[0, 0])\n",
    "        buffer[\"log_probs\"].append(log_prob)\n",
    "\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            episode_return = 0\n",
    "            episode_length = 0\n",
    "        else:\n",
    "            obs = next_obs\n",
    "        \n",
    "        observations = {}\n",
    "        for key, val in buffer.items():\n",
    "            observations[key] = jnp.array(val)\n",
    "        \n",
    "        last_value = state.apply_fn(state.params, jnp.array([obs]))[2][0, 0]\n",
    "        advantages, returns = compute_advantages(\n",
    "            observations[\"rewards\"],\n",
    "            observations[\"values\"],\n",
    "            observations[\"dones\"],\n",
    "            last_value,\n",
    "            gamma\n",
    "        )\n",
    "        observations[\"advantages\"] = advantages\n",
    "        observations[\"returns\"] = returns\n",
    "        return observations, rng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    env,\n",
    "    seed,\n",
    "    num_epochs,\n",
    "    steps_per_epoch,\n",
    "    batch_size,\n",
    "    num_minibatches,\n",
    "    gamma,\n",
    "    clip_eps,\n",
    "    learning_rate,\n",
    "    max_grad_norm\n",
    "):\n",
    "    dummy_state = env.reset()\n",
    "    input_dim = dummy_state.shape\n",
    "    action_dim = env.action_dim()\n",
    "\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    rng, actor_rng = jax.random.split(rng)\n",
    "    actor_critic = ActorCritic(action_dim)\n",
    "    state = create_train_state(rng, actor_critic, input_dim, action_dim, learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        obs, rng = collect_trajectories(\n",
    "            env, state, rng, steps_per_epoch, gamma\n",
    "        )\n",
    "        state = update_ppo(state, obs, batch_size, num_minibatches, clip_eps, max_grad_norm)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            eval_returns, sample = evaluate_policy(env, state, rng, 1)\n",
    "            print(\"Eval return {eval_returns}, Sample {sample}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, state, rng, evals):\n",
    "    returns = []\n",
    "    samples = []\n",
    "    for _ in range(evals):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_return = 0\n",
    "        while not done:\n",
    "            mean, _ = state.apply_fn(state.params, jnp.array([obs]))\n",
    "            action = np.array(sample_actions(rng, mean))\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            episode_return += reward\n",
    "        sample = env.get_sample()\n",
    "        returns.append(episode_return)\n",
    "        samples.append(sample)\n",
    "    return np.mean(np.array(returns)), samples[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    state = train(\n",
    "        env,\n",
    "        42,\n",
    "        200,\n",
    "        2048,\n",
    "        64,\n",
    "        64,\n",
    "        0.99,\n",
    "        0.2,\n",
    "        3e-4,\n",
    "        0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to complete\n",
    "- [x] discrete actions\n",
    "- [x] advantage computation\n",
    "- [ ] environment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
