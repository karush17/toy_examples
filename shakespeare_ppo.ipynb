{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5bZiIsfRSaR"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTyQBUiRRiE8",
        "outputId": "d9886b90-e47a-4392-b07c-424cd4365b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from flax.training import train_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQOTLNDWRSaT"
      },
      "source": [
        "## Actor-Critic Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "161spFeRReYu"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    action_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, state):\n",
        "      x = nn.Dense(64)(state)\n",
        "      x = nn.gelu(x)\n",
        "      x = nn.Dense(64)(x)\n",
        "      x = nn.gelu(x)\n",
        "\n",
        "      logits = nn.Dense(self.action_dim)(x)\n",
        "      value = nn.Dense(1)(x)\n",
        "\n",
        "      return logits, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhEh6N6pRSaT"
      },
      "source": [
        "## Sample Actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JPOdl23vRSaU",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def sample_actions(rng, logits, random = True):\n",
        "    log_probs = jax.nn.log_softmax(logits)\n",
        "    action = chosen_action = jax.random.categorical(rng, logits)\n",
        "    if random:\n",
        "      if jax.random.uniform(rng, shape=(1,), minval = 0, maxval = 1)[0] > 0.5:\n",
        "        action = jax.random.randint(rng, shape=(1,), minval = 0, maxval = len(logits[0]))\n",
        "      else:\n",
        "        action = chosen_action\n",
        "    else:\n",
        "      action = chosen_action\n",
        "    return action, log_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebnsaAapRSaU"
      },
      "source": [
        "## Compute Advantages and Reward-To-Go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kGKaSDS_RSaU",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def compute_advantages(rewards, values, dones, last_value, gamma):\n",
        "    returns = []\n",
        "    discounted_reward = 0\n",
        "    for i in reversed(range(len(rewards))):\n",
        "        discounted_reward = rewards[i] + gamma * discounted_reward * dones[i]\n",
        "        returns.insert(0, discounted_reward)\n",
        "    returns = np.array(returns)\n",
        "    advantages = returns - values\n",
        "    return advantages, returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHsAcTMkRSaV"
      },
      "source": [
        "## Create Train State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "idUTeL7ERSaV",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, model, input_dim, action_dim, learning_rate):\n",
        "    dummy_input = jnp.ones((1, input_dim))\n",
        "    params = model.init(rng, dummy_input)\n",
        "    tx = optax.adam(learning_rate)\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        params = params,\n",
        "        tx = tx\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRuh2wAVRSaV"
      },
      "source": [
        "## Compute Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_QT-RchQRSaV",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "@functools.partial(jax.jit, static_argnums = (6, 7))\n",
        "def train_step(\n",
        "    state,\n",
        "    states,\n",
        "    actions,\n",
        "    old_log_probs,\n",
        "    advantages,\n",
        "    returns,\n",
        "    clip_eps,\n",
        "    max_grad_norm\n",
        "):\n",
        "    def loss_fn(params):\n",
        "        mean, values = state.apply_fn(params, states)\n",
        "        log_probs = jax.nn.log_softmax(mean)\n",
        "\n",
        "        ratio = jnp.exp(log_probs - old_log_probs)\n",
        "        clipped_ratio = jnp.clip(ratio, 1 - clip_eps, 1 + clip_eps)\n",
        "        loss1 = ratio * advantages\n",
        "        loss2 = clipped_ratio * advantages\n",
        "        policy_loss = -jnp.mean(jnp.minimum(loss1, loss2))\n",
        "\n",
        "        value_loss = jnp.mean((values.squeeze() - returns) ** 2)\n",
        "\n",
        "        return policy_loss + value_loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    _, grads = grad_fn(state.params)\n",
        "    # grads, _ = optax.clip_by_global_norm(grads, max_grad_norm)\n",
        "    new_state = state.apply_gradients(grads = grads)\n",
        "    return new_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwDWvhM9RSaV"
      },
      "source": [
        "## Update Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xDgub0IcRSaV",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def update_ppo(state, obs, batch_size, num_minibatches, clip_eps, max_grad_norm):\n",
        "    indices = jnp.arange(len(obs[\"states\"]))\n",
        "    indices = jax.random.permutation(jax.random.PRNGKey(42), indices)\n",
        "\n",
        "    for _ in range(num_minibatches):\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            mb_indices = indices[i: i + batch_size]\n",
        "            mb_states = obs[\"states\"][mb_indices]\n",
        "            mb_actions = obs[\"actions\"][mb_indices]\n",
        "            mb_old_logprobs = obs[\"log_probs\"][mb_indices]\n",
        "            mb_advantages = obs[\"advantages\"][mb_indices]\n",
        "            mb_returns = obs[\"returns\"][mb_indices]\n",
        "\n",
        "            mb_advantages = (mb_advantages - jnp.mean(mb_advantages)) / jnp.std(mb_advantages) + 1e-8\n",
        "            state = train_step(state, mb_states, mb_actions, mb_old_logprobs, mb_advantages, mb_returns, clip_eps, max_grad_norm)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Jz-OBbRSaV"
      },
      "source": [
        "## Collect Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yHuIZuUmRSaV",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def collect_trajectories(env, state, rng, steps_per_epoch, gamma):\n",
        "    buffer = {\n",
        "        \"states\": [],\n",
        "        \"actions\": [],\n",
        "        \"rewards\": [],\n",
        "        \"dones\": [],\n",
        "        \"values\": [],\n",
        "        \"log_probs\": []\n",
        "    }\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    episode_return = 0\n",
        "    episode_length = 0\n",
        "    for _ in range(steps_per_epoch):\n",
        "        rng, actions_rng = jax.random.split(rng)\n",
        "        mean, value = state.apply_fn(state.params, jnp.array([obs]))\n",
        "        action, log_prob = sample_actions(actions_rng, mean, random = True)\n",
        "\n",
        "        next_obs, reward, done, _ = env.step(np.array(action))\n",
        "\n",
        "        buffer[\"states\"].append(obs)\n",
        "        buffer[\"actions\"].append(action)\n",
        "        buffer[\"rewards\"].append(reward)\n",
        "        buffer[\"dones\"].append(done)\n",
        "        buffer[\"values\"].append(value[0, 0])\n",
        "        buffer[\"log_probs\"].append(log_prob.squeeze(0))\n",
        "\n",
        "        episode_return += reward\n",
        "        episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            episode_return = 0\n",
        "            episode_length = 0\n",
        "        else:\n",
        "            obs = next_obs\n",
        "\n",
        "    observations = {}\n",
        "    for key, val in buffer.items():\n",
        "        observations[key] = jnp.array(val)\n",
        "\n",
        "    last_value = state.apply_fn(state.params, jnp.array([obs]))[1][0, 0]\n",
        "    advantages, returns = compute_advantages(\n",
        "        observations[\"rewards\"],\n",
        "        observations[\"values\"],\n",
        "        observations[\"dones\"],\n",
        "        last_value,\n",
        "        gamma\n",
        "    )\n",
        "    observations[\"advantages\"] = advantages[:, jnp.newaxis]\n",
        "    observations[\"returns\"] = returns[:, jnp.newaxis]\n",
        "    return observations, rng\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIa5SZxiRSaV"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hqis9pYFRSaV",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    env,\n",
        "    seed,\n",
        "    num_epochs,\n",
        "    steps_per_epoch,\n",
        "    batch_size,\n",
        "    num_minibatches,\n",
        "    gamma,\n",
        "    clip_eps,\n",
        "    learning_rate,\n",
        "    max_grad_norm\n",
        "):\n",
        "    dummy_state = env.reset()\n",
        "    input_dim = dummy_state.shape[0]\n",
        "    action_dim = env.action_dim()\n",
        "\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    rng, actor_rng = jax.random.split(rng)\n",
        "    actor_critic = ActorCritic(action_dim)\n",
        "    state = create_train_state(rng, actor_critic, input_dim, action_dim, learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        obs, rng = collect_trajectories(\n",
        "            env, state, rng, steps_per_epoch, gamma\n",
        "        )\n",
        "        state = update_ppo(state, obs, batch_size, num_minibatches, clip_eps, max_grad_norm)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            eval_returns, sample = evaluate_policy(env, state, rng, 1)\n",
        "            print(f\"Eval return {eval_returns}, Sample {sample}\")\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Uig8EwRSaV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "617r4NhuRSaV",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(env, state, rng, evals):\n",
        "    returns = []\n",
        "    samples = []\n",
        "    for _ in range(evals):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        episode_return = 0\n",
        "        while not done:\n",
        "            mean, _ = state.apply_fn(state.params, jnp.array([obs]))\n",
        "            action = np.array(sample_actions(rng, mean, random = False)[0])\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            episode_return += reward\n",
        "        sample = env.get_sample()\n",
        "        returns.append(episode_return)\n",
        "        samples.append(sample)\n",
        "    return np.mean(np.array(returns)), samples[-1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpvitfVVRSaW"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tenVbdPZRSaW",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "\n",
        "    def __init__(self):\n",
        "      self.pros = \"Shall I compare thee to a summer's day ? , Thou art more lovely and more temperate\"\n",
        "      self.tokens = self.pros.split(\" \")\n",
        "      self.pro_length = 17\n",
        "      self.words_to_actions = {\n",
        "          \"I\": 1,\n",
        "          \"compare\": 2,\n",
        "          \"thee\": 3,\n",
        "          \"to\": 4,\n",
        "          \"a\": 5,\n",
        "          \"summer's\": 6,\n",
        "          \"day\": 7,\n",
        "          \"thou\": 8,\n",
        "          \"art\": 9,\n",
        "          \"more\": 10,\n",
        "          \"lovely\": 11,\n",
        "          \"and\": 12,\n",
        "          \"temperate\": 13,\n",
        "          \",\": 14,\n",
        "          \"?\": 15,\n",
        "          \"Shall\": 16\n",
        "      }\n",
        "      self.actions_to_words = {v:k for k, v in self.words_to_actions.items()}\n",
        "      self.state = np.zeros((self.pro_length,))\n",
        "\n",
        "    def action_dim(self):\n",
        "      return len(self.words_to_actions)\n",
        "\n",
        "    def get_sample(self):\n",
        "      return self.current_sample\n",
        "\n",
        "    def reset(self):\n",
        "      self.state = np.zeros((self.pro_length,))\n",
        "      self.counter = 0\n",
        "      self.current_sample = \"\"\n",
        "      return self.state\n",
        "\n",
        "    def get_reward(self):\n",
        "      prediction = set(self.current_sample)\n",
        "      ground_truth = set(\"\".join(self.tokens[:len(prediction)]))\n",
        "      score = 1 - jaccard_distance(prediction, ground_truth)\n",
        "      return score\n",
        "\n",
        "    def step(self, action):\n",
        "      action = action[0]\n",
        "      word = self.actions_to_words[action + 1]\n",
        "      self.state[self.counter] = action\n",
        "      self.current_sample = self.current_sample + \" \" + word\n",
        "      reward = self.get_reward()\n",
        "      self.counter += 1\n",
        "      if self.counter != self.pro_length:\n",
        "        done = False\n",
        "      else:\n",
        "        done = True\n",
        "      return self.state, reward, done, None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-92O2OiRSaW"
      },
      "source": [
        "## Main Runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h_2aoY8RSaW",
        "outputId": "55d19e84-984f-4ccc-e6af-04b25d7968db",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval return 6.668478260869566, Sample  temperate temperate temperate temperate temperate and and and and and lovely and lovely lovely lovely lovely lovely\n",
            "Eval return 6.464285714285717, Sample  summer's summer's and and and and and and and and and and and and and and and\n",
            "Eval return 2.8333333333333317, Sample  I I I I I I I I I I I I I I I I I\n",
            "Eval return 1.0708333333333333, Sample  day day and and and and and and and and and and and and and and and\n",
            "Eval return 1.133333333333333, Sample  and and and and and and and and and and and and and and and and and\n",
            "Eval return 4.77142857142857, Sample  lovely and and and and and and and and and and and and and and and and\n",
            "Eval return 2.8333333333333317, Sample  I I I I I I I I I I I I I I I I I\n",
            "Eval return 2.8333333333333317, Sample  I I I I I I I I I I I I I I I I I\n",
            "Eval return 2.8333333333333317, Sample  I I I I I I I I I I I I I I I I I\n",
            "Eval return 2.8333333333333317, Sample  I I I I I I I I I I I I I I I I I\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env = Environment()\n",
        "    state = train(\n",
        "        env,\n",
        "        42,\n",
        "        100,\n",
        "        1024,\n",
        "        64,\n",
        "        64,\n",
        "        0.99,\n",
        "        0.2,\n",
        "        3e-4,\n",
        "        0.5\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B8n-2S3YJmD"
      },
      "source": [
        "## AI Generated Solution (for reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpg9nQnbKOrY",
        "outputId": "ff204bd4-af51-43ee-d082-150c72c77931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training PPO agent to overfit Shakespeare poem...\n",
            "Step 0/100, Generated text:\n",
            "\n",
            "Shall I cshthkRR\n",
            "BR' xRRtoW:sf :xNcu' tktb,pkxebuRtasne:?vgIsppmoIm,AAi.nA:ecaBwiIulnx\n",
            ",:iy  mxhT,pW.tNfbpMb:\n",
            "----------------------------------------\n",
            "Step 10/100, Generated text:\n",
            "\n",
            "Shall I cf?.bmwm;\n",
            ".el'bRrgys:N?.:bh:omgRhNfrd;svBf?.v?Mmri.f. lN ,rd?mxnlNfxB,iSTB;;pnkn;Rug:I?ehxArNfyrg;R A\n",
            "----------------------------------------\n",
            "Step 20/100, Generated text:\n",
            "\n",
            "Shall I cWMWTMBTMR.vrI?MsMT\n",
            "RwWT;A:bMbfNbIvcT.xn'IW,AMtM?wMSwM?B.fmwfivvTh,wW?;xn.p?:rWuS?WRNwBNbwMMNd:s:?:;A\n",
            "----------------------------------------\n",
            "Step 30/100, Generated text:\n",
            "\n",
            "Shall I cMTTTBTBBBmmg.S?emmyThwAi whcoWWBB?BdI;aesNiavxBBBRBSThn pdfBATTT?WIhfNRTlr:ITBkRTSBmmdpmyig;nbfyMBBA\n",
            "----------------------------------------\n",
            "Step 40/100, Generated text:\n",
            "\n",
            "Shall I cTBTTTTBTTB::\n",
            "ts:gAM?W?ABTW?BBT?B\n",
            "WMAf BTBWSTB?TBvN.BWAlB?'TAWxBWBTNB\n",
            "WBe;WNWWBBTB?TBN:onANWBxTxATTNx\n",
            "----------------------------------------\n",
            "Step 50/100, Generated text:\n",
            "\n",
            "Shall I cBBBBATBTT??BT?WTRBABBRTB?BANNTWxTWTTWTB?AWB??WTTBWBTBATTWRTBTBTTWWTTTWWWTTTT?xTTkIRA??W:TWRATATTTNMW\n",
            "----------------------------------------\n",
            "Step 60/100, Generated text:\n",
            "\n",
            "Shall I cWWWTWAWTTWATTATATTWATTTTWATTWTTATTWTTATAWATTTTTTTTTTBTTTAAATTTTTTTTTAWTWTWTATTATAAT?WAATTTWTWTWTTTTT\n",
            "----------------------------------------\n",
            "Step 70/100, Generated text:\n",
            "\n",
            "Shall I cAATTTATTBAATATTATTW?TAWAAWTWTWTAAWTTAATAWTAAAWAAWAWTAWATAAATT?WTAWTATAAATATTAAWTAWTTAAAAWAATAWTW?WAW\n",
            "----------------------------------------\n",
            "Step 80/100, Generated text:\n",
            "\n",
            "Shall I cAATAAAA?AATATWATWAAAWATATAAAAAAAATATTAAWWAAAAAAAWATA?TTTWAATATAATAATWTTAWA??AWATAATATTTAAAAWATWATTAA\n",
            "----------------------------------------\n",
            "Step 90/100, Generated text:\n",
            "\n",
            "Shall I cAAAAAAAAAAAWAAAAWAAWAAAAAATAAA?AAAAAAWAWAAAWAAAAAAATTAAAAAAAA?AAAAAWAAATAAAWAAAAAAAAWWAAATAAAAAAWAAA\n",
            "----------------------------------------\n",
            "\n",
            "Final generated text:\n",
            "\n",
            "Shall I cAAAIrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "\n",
            "Evaluating overfitting performance...\n",
            "Character prediction accuracy: 10.00%\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "from flax.training.train_state import TrainState\n",
        "import numpy as np\n",
        "from typing import Any, Dict, List, Tuple\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "key = jax.random.PRNGKey(SEED)\n",
        "\n",
        "# Shakespeare poem to overfit\n",
        "SHAKESPEARE_POEM = \"\"\"\n",
        "Shall I compare thee to a summer's day?\n",
        "Thou art more lovely and more temperate:\n",
        "Rough winds do shake the darling buds of May,\n",
        "And summer's lease hath all too short a date;\n",
        "Sometime too hot the eye of heaven shines,\n",
        "And often is his gold complexion dimm'd;\n",
        "And every fair from fair sometime declines,\n",
        "By chance or nature's changing course untrimm'd;\n",
        "But thy eternal summer shall not fade,\n",
        "Nor lose possession of that fair thou ow'st;\n",
        "Nor shall death brag thou wander'st in his shade,\n",
        "When in eternal lines to time thou grow'st:\n",
        "So long as men can breathe or eyes can see,\n",
        "So long lives this, and this gives life to thee.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenization helpers\n",
        "def create_vocabulary(text):\n",
        "    # Create a simple character-level vocabulary\n",
        "    chars = sorted(list(set(text)))\n",
        "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "    vocab_size = len(chars)\n",
        "    return char_to_idx, idx_to_char, vocab_size\n",
        "\n",
        "def encode_text(text, char_to_idx):\n",
        "    return [char_to_idx[ch] for ch in text]\n",
        "\n",
        "def decode_tokens(tokens, idx_to_char):\n",
        "    return ''.join([idx_to_char[idx] for idx in tokens])\n",
        "\n",
        "# Create vocabulary from the poem\n",
        "char_to_idx, idx_to_char, vocab_size = create_vocabulary(SHAKESPEARE_POEM)\n",
        "encoded_poem = encode_text(SHAKESPEARE_POEM, char_to_idx)\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "CONTEXT_LENGTH = 10\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 3e-4\n",
        "PPO_EPOCHS = 4\n",
        "NUM_STEPS = 100\n",
        "CLIP_RATIO = 0.2\n",
        "VALUE_COEF = 0.5\n",
        "ENTROPY_COEF = 0.01\n",
        "GAE_LAMBDA = 0.95\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Define the Actor-Critic model with Flax\n",
        "class ActorCritic(nn.Module):\n",
        "    vocab_size: int\n",
        "    emb_dim: int\n",
        "    hidden_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # Shared embedding layer\n",
        "        emb = nn.Embed(self.vocab_size, self.emb_dim)(x)\n",
        "\n",
        "        # LSTM for sequential processing\n",
        "        lstm_out = nn.RNN(nn.LSTMCell(self.hidden_dim))(emb)\n",
        "\n",
        "        # Actor head (policy)\n",
        "        logits = nn.Dense(self.vocab_size)(lstm_out)\n",
        "\n",
        "        # Critic head (value function)\n",
        "        value = nn.Dense(1)(lstm_out)\n",
        "\n",
        "        return logits, value\n",
        "\n",
        "# Initialize model and optimizer\n",
        "def init_model(key, vocab_size):\n",
        "    model = ActorCritic(vocab_size=vocab_size, emb_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
        "\n",
        "    # Create a sample input\n",
        "    dummy_input = jnp.zeros((1, CONTEXT_LENGTH), dtype=jnp.int32)\n",
        "\n",
        "    # Initialize parameters\n",
        "    params = model.init(key, dummy_input)\n",
        "\n",
        "    # Create optimizer\n",
        "    tx = optax.adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "    # Create train state\n",
        "    return TrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=tx\n",
        "    )\n",
        "\n",
        "# Generate data for training\n",
        "def get_batch(encoded_text, batch_size, context_length):\n",
        "    # Choose random starting indices\n",
        "    max_idx = len(encoded_text) - context_length - 1\n",
        "    indices = np.random.randint(0, max_idx, size=batch_size)\n",
        "\n",
        "    # Create x and y batches\n",
        "    x_batch = np.array([encoded_text[idx:idx+context_length] for idx in indices])\n",
        "    y_batch = np.array([encoded_text[idx+1:idx+context_length+1] for idx in indices])\n",
        "\n",
        "    return x_batch, y_batch\n",
        "\n",
        "# PPO loss function\n",
        "@functools.partial(jax.jit)\n",
        "def ppo_loss(state, states, actions, rewards, old_log_probs, advantages):\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Forward pass with current parameters\n",
        "        logits, values = state.apply_fn(params, states)\n",
        "\n",
        "        # Calculate policy loss (PPO-Clip)\n",
        "        log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "        action_log_probs = jnp.take_along_axis(log_probs, actions[:, :, None], axis=-1).squeeze(-1)\n",
        "\n",
        "        # Compute ratio and clipped ratio\n",
        "        ratio = jnp.exp(action_log_probs - old_log_probs)\n",
        "        clipped_ratio = jnp.clip(ratio, 1 - CLIP_RATIO, 1 + CLIP_RATIO)\n",
        "\n",
        "        # Policy loss\n",
        "        policy_loss = -jnp.mean(jnp.minimum(ratio * advantages, clipped_ratio * advantages))\n",
        "\n",
        "        # Value loss\n",
        "        value_loss = jnp.mean(jnp.square(values.squeeze(-1) - rewards))\n",
        "\n",
        "        # Entropy for exploration\n",
        "        entropy = jnp.mean(-jnp.sum(jnp.exp(log_probs) * log_probs, axis=-1))\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    # Update model with PPO loss\n",
        "    grad_fn = jax.value_and_grad(loss_fn, allow_int = True)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state\n",
        "\n",
        "\n",
        "# Compute GAE (Generalized Advantage Estimation)\n",
        "def compute_gae(rewards, values, next_values, dones, gamma=GAMMA, lam=GAE_LAMBDA):\n",
        "    advantages = []\n",
        "    gae = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        if t == len(rewards) - 1:\n",
        "            next_value = next_values[t]\n",
        "        else:\n",
        "            next_value = values[t + 1]\n",
        "\n",
        "        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]\n",
        "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
        "        advantages.insert(0, gae)\n",
        "\n",
        "    advantages = jnp.array(advantages)\n",
        "    returns = advantages + jnp.array(values)\n",
        "\n",
        "    # Normalize advantages\n",
        "    advantages = (advantages - jnp.mean(advantages)) / (jnp.std(advantages) + 1e-8)\n",
        "\n",
        "    return advantages, returns\n",
        "\n",
        "# Training function\n",
        "def train_ppo():\n",
        "    # Initialize model\n",
        "    key, subkey = jax.random.split(jax.random.PRNGKey(SEED))\n",
        "    state = init_model(subkey, vocab_size)\n",
        "\n",
        "    for step in range(NUM_STEPS):\n",
        "        # Sample batch\n",
        "        states, next_states = get_batch(encoded_poem, BATCH_SIZE, CONTEXT_LENGTH)\n",
        "        states = jnp.array(states)\n",
        "        actions = jnp.array(next_states)\n",
        "\n",
        "        # Get current policy and value predictions\n",
        "        logits, values = state.apply_fn(state.params, states)\n",
        "\n",
        "        # Sample actions and compute log probabilities\n",
        "        log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
        "        action_log_probs = jnp.take_along_axis(log_probs, actions[:, :, None], axis=-1).squeeze(-1)\n",
        "\n",
        "        # Simple reward: match the target poem\n",
        "        rewards = jnp.sum(jnp.equal(jnp.argmax(logits, axis=-1), actions), axis=-1) / CONTEXT_LENGTH\n",
        "\n",
        "        # Compute advantages and returns\n",
        "        dones = jnp.zeros_like(rewards)  # No episode termination in this task\n",
        "        next_values = values  # Simple approximation\n",
        "        advantages, returns = compute_gae(rewards, values.squeeze(-1), next_values.squeeze(-1), dones)\n",
        "\n",
        "        # PPO update loop\n",
        "        for _ in range(PPO_EPOCHS):\n",
        "            # Update model with PPO loss\n",
        "            state = ppo_loss(state, states, actions, returns, action_log_probs, advantages)\n",
        "\n",
        "        # Print progress and generated text occasionally\n",
        "        if step % 10 == 0:\n",
        "            # Generate text from the model\n",
        "            generated_text = generate_text(state, 100)\n",
        "            print(f\"Step {step}/{NUM_STEPS}, Generated text:\")\n",
        "            print(generated_text)\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    return state\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(state, length, temperature=0.8, prompt=None):\n",
        "    if prompt is None:\n",
        "        # Start with the first few characters from the poem\n",
        "        prompt = SHAKESPEARE_POEM[:10]\n",
        "        tokens = encode_text(prompt, char_to_idx)\n",
        "    else:\n",
        "        tokens = encode_text(prompt, char_to_idx)\n",
        "\n",
        "    # Pad the context if needed\n",
        "    if len(tokens) < CONTEXT_LENGTH:\n",
        "        tokens = [0] * (CONTEXT_LENGTH - len(tokens)) + tokens\n",
        "    elif len(tokens) > CONTEXT_LENGTH:\n",
        "        tokens = tokens[-CONTEXT_LENGTH:]\n",
        "\n",
        "    context = jnp.array([tokens])\n",
        "    generated = list(tokens)\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Get logits from the model\n",
        "        logits, _ = state.apply_fn(state.params, context)\n",
        "\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Sample from the distribution\n",
        "        next_token = jax.random.categorical(jax.random.PRNGKey(random.randint(0, 10000)),\n",
        "                                           logits[0, -1, :])\n",
        "\n",
        "        # Append to generated text\n",
        "        generated.append(int(next_token))\n",
        "\n",
        "        # Update context\n",
        "        context = jnp.roll(context, -1, axis=1)\n",
        "        context = context.at[:, -1].set(int(next_token))\n",
        "\n",
        "    # Decode and return\n",
        "    return decode_tokens(generated, idx_to_char)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training PPO agent to overfit Shakespeare poem...\")\n",
        "    final_state = train_ppo()\n",
        "\n",
        "    print(\"\\nFinal generated text:\")\n",
        "    print(generate_text(final_state, 200, temperature=0.5))\n",
        "\n",
        "    # Evaluate overfitting performance\n",
        "    print(\"\\nEvaluating overfitting performance...\")\n",
        "    states, next_states = get_batch(encoded_poem, 1, CONTEXT_LENGTH)\n",
        "    logits, _ = final_state.apply_fn(final_state.params, jnp.array(states))\n",
        "    predictions = jnp.argmax(logits, axis=-1)\n",
        "    accuracy = jnp.mean(jnp.equal(predictions, jnp.array(next_states)))\n",
        "    print(f\"Character prediction accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xObr1vEkYLzL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
